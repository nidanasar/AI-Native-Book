"use strict";(globalThis.webpackChunkhackathon_text_book=globalThis.webpackChunkhackathon_text_book||[]).push([[6310],{3741:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-2-digital-twin/simulating-sensors","title":"Chapter 3: Simulating Sensors for Humanoid Robots","description":"Add and configure virtual sensors including LiDAR, depth cameras, and IMUs","source":"@site/docs/module-2-digital-twin/03-simulating-sensors.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/simulating-sensors","permalink":"/docs/module-2-digital-twin/simulating-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-digital-twin/03-simulating-sensors.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 3: Simulating Sensors for Humanoid Robots","description":"Add and configure virtual sensors including LiDAR, depth cameras, and IMUs"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Physics Simulation with Gazebo","permalink":"/docs/module-2-digital-twin/physics-simulation-gazebo"},"next":{"title":"Chapter 4: High-Fidelity Visualization and Interaction with Unity","permalink":"/docs/module-2-digital-twin/unity-visualization-hri"}}');var r=i(4848),a=i(8453);const t={sidebar_position:3,title:"Chapter 3: Simulating Sensors for Humanoid Robots",description:"Add and configure virtual sensors including LiDAR, depth cameras, and IMUs"},l="Simulating Sensors for Humanoid Robots",o={},d=[{value:"Why Simulate Sensors?",id:"why-simulate-sensors",level:2},{value:"The Perception-Action Loop",id:"the-perception-action-loop",level:3},{value:"Virtual Rehearsal for Sensing",id:"virtual-rehearsal-for-sensing",level:3},{value:"Benefits of Sensor Simulation",id:"benefits-of-sensor-simulation",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"How LiDAR Works",id:"how-lidar-works",level:3},{value:"Adding LiDAR to Your Robot",id:"adding-lidar-to-your-robot",level:3},{value:"LiDAR Configuration Parameters",id:"lidar-configuration-parameters",level:3},{value:"Bridging LiDAR Data to ROS 2",id:"bridging-lidar-data-to-ros-2",level:3},{value:"Visualizing in RViz",id:"visualizing-in-rviz",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"How Depth Cameras Work",id:"how-depth-cameras-work",level:3},{value:"Adding a Depth Camera",id:"adding-a-depth-camera",level:3},{value:"Depth Camera Topics",id:"depth-camera-topics",level:3},{value:"Bridge Configuration",id:"bridge-configuration",level:3},{value:"Viewing Point Clouds",id:"viewing-point-clouds",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"What an IMU Measures",id:"what-an-imu-measures",level:3},{value:"Adding an IMU Sensor",id:"adding-an-imu-sensor",level:3},{value:"IMU Data in ROS 2",id:"imu-data-in-ros-2",level:3},{value:"Using IMU for Balance",id:"using-imu-for-balance",level:3},{value:"Sensor Noise and Realism",id:"sensor-noise-and-realism",level:2},{value:"Types of Sensor Noise",id:"types-of-sensor-noise",level:3},{value:"Configuring Noise in Gazebo",id:"configuring-noise-in-gazebo",level:3},{value:"Domain Randomization for Sensors",id:"domain-randomization-for-sensors",level:3},{value:"The Sim-to-Real Sensor Gap",id:"the-sim-to-real-sensor-gap",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"simulating-sensors-for-humanoid-robots",children:"Simulating Sensors for Humanoid Robots"})}),"\n",(0,r.jsxs)(n.admonition,{title:"Learning Objectives",type:"info",children:[(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Add simulated sensors to your robot model in Gazebo"}),"\n",(0,r.jsx)(n.li,{children:"Configure LiDAR sensors and visualize laser scan data in RViz"}),"\n",(0,r.jsx)(n.li,{children:"Set up depth cameras and view point cloud output"}),"\n",(0,r.jsx)(n.li,{children:"Implement IMU sensors for orientation and acceleration data"}),"\n",(0,r.jsx)(n.li,{children:"Understand sensor noise models and their importance for sim-to-real transfer"}),"\n"]})]}),"\n",(0,r.jsx)(n.h2,{id:"why-simulate-sensors",children:"Why Simulate Sensors?"}),"\n",(0,r.jsxs)(n.p,{children:["In Chapter 2, you gave your robot a body that obeys physics\u2014it can move, fall, and collide. But a body without senses is useless. Your robot needs to ",(0,r.jsx)(n.em,{children:"perceive"})," its environment to act intelligently. This is where ",(0,r.jsx)(n.strong,{children:"sensor simulation"})," completes the digital twin."]}),"\n",(0,r.jsx)(n.h3,{id:"the-perception-action-loop",children:"The Perception-Action Loop"}),"\n",(0,r.jsx)(n.p,{children:"Every intelligent robot operates through a perception-action loop:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    THE ROBOT LOOP                         \u2502\r\n\u2502                                                           \u2502\r\n\u2502   Sense \u2500\u2500\u25ba Process \u2500\u2500\u25ba Decide \u2500\u2500\u25ba Act \u2500\u2500\u25ba (repeat)      \u2502\r\n\u2502     \u2502                                  \u2502                  \u2502\r\n\u2502     \u2502         Environment              \u2502                  \u2502\r\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.p,{children:'Without simulated sensors, you can only test the "Act" part of this loop. Sensor simulation lets you develop and test the entire perception pipeline:'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computer vision"})," algorithms processing camera data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SLAM"})," (Simultaneous Localization and Mapping) using LiDAR"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State estimation"})," combining IMU with other sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Obstacle detection"})," for navigation and safety"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"virtual-rehearsal-for-sensing",children:"Virtual Rehearsal for Sensing"}),"\n",(0,r.jsxs)(n.p,{children:["Continuing our virtual rehearsal analogy: when you imagine catching a ball, you don't just simulate your arm movement\u2014you also imagine ",(0,r.jsx)(n.em,{children:"seeing"})," the ball approach and ",(0,r.jsx)(n.em,{children:"feeling"})," it hit your hand. Your mental rehearsal includes predicted sensory feedback."]}),"\n",(0,r.jsx)(n.p,{children:"Similarly, a complete digital twin needs simulated sensors:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Human Mental Rehearsal"}),(0,r.jsx)(n.th,{children:"Robot Sensor Simulation"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Imagined sight"}),(0,r.jsx)(n.td,{children:"Simulated camera/LiDAR"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Imagined proprioception"}),(0,r.jsx)(n.td,{children:"Simulated joint encoders"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Imagined balance"}),(0,r.jsx)(n.td,{children:"Simulated IMU"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Imagined touch"}),(0,r.jsx)(n.td,{children:"Simulated contact sensors"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"benefits-of-sensor-simulation",children:"Benefits of Sensor Simulation"}),"\n",(0,r.jsx)(n.p,{children:"Simulated sensors provide unique advantages:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unlimited data generation"}),": Train ML models on millions of images without manual labeling"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ground truth access"}),": Know the exact position of every object (impossible in reality)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Controllable conditions"}),": Test perception in rain, fog, darkness\u2014on demand"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safe failure testing"}),": What happens if a sensor fails mid-operation?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rapid iteration"}),": Adjust sensor placement instantly without hardware changes"]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"Reality Check",type:"warning",children:(0,r.jsx)(n.p,{children:'Simulated sensors produce "perfect" data with optional artificial noise. Real sensors have complex, correlated noise patterns that are hard to model. We\'ll discuss bridging this gap in the final section.'})}),"\n",(0,r.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"})," (Light Detection and Ranging) measures distances by bouncing laser beams off surfaces. It's essential for humanoid robots navigating complex environments, providing 360\xb0 awareness that cameras can't match."]}),"\n",(0,r.jsx)(n.h3,{id:"how-lidar-works",children:"How LiDAR Works"}),"\n",(0,r.jsx)(n.p,{children:"A LiDAR sensor:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Emits laser pulses in multiple directions"}),"\n",(0,r.jsx)(n.li,{children:"Measures the time for each pulse to return"}),"\n",(0,r.jsxs)(n.li,{children:["Calculates distance: ",(0,r.jsx)(n.code,{children:"distance = (time \xd7 speed_of_light) / 2"})]}),"\n",(0,r.jsx)(n.li,{children:"Produces a point cloud or laser scan"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"For humanoid robots, 2D LiDAR (horizontal plane) is common for navigation, while 3D LiDAR provides richer environmental understanding."}),"\n",(0,r.jsx)(n.h3,{id:"adding-lidar-to-your-robot",children:"Adding LiDAR to Your Robot"}),"\n",(0,r.jsx)(n.p,{children:"In Gazebo, add a LiDAR sensor using the GPU-accelerated plugin:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Add inside a link in your URDF (Gazebo-specific section) --\x3e\r\n<gazebo reference="head_link">\r\n  <sensor name="head_lidar" type="gpu_lidar">\r\n    <pose>0 0 0.1 0 0 0</pose>  \x3c!-- Position relative to head --\x3e\r\n    <topic>scan</topic>\r\n    <update_rate>10</update_rate>\r\n\r\n    <lidar>\r\n      <scan>\r\n        <horizontal>\r\n          <samples>640</samples>\r\n          <resolution>1</resolution>\r\n          <min_angle>-1.5708</min_angle>   \x3c!-- -90 degrees --\x3e\r\n          <max_angle>1.5708</max_angle>    \x3c!-- +90 degrees --\x3e\r\n        </horizontal>\r\n        <vertical>\r\n          <samples>1</samples>   \x3c!-- 2D LiDAR: single plane --\x3e\r\n          <resolution>1</resolution>\r\n          <min_angle>0</min_angle>\r\n          <max_angle>0</max_angle>\r\n        </vertical>\r\n      </scan>\r\n      <range>\r\n        <min>0.1</min>    \x3c!-- Minimum detection range (meters) --\x3e\r\n        <max>30.0</max>   \x3c!-- Maximum detection range (meters) --\x3e\r\n        <resolution>0.01</resolution>\r\n      </range>\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.01</stddev>  \x3c!-- 1cm standard deviation --\x3e\r\n      </noise>\r\n    </lidar>\r\n\r\n    <always_on>true</always_on>\r\n    <visualize>true</visualize>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"lidar-configuration-parameters",children:"LiDAR Configuration Parameters"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Typical Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"samples"})}),(0,r.jsx)(n.td,{children:"Number of beams per scan"}),(0,r.jsx)(n.td,{children:"360-1080"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"min_angle"}),"/",(0,r.jsx)(n.code,{children:"max_angle"})]}),(0,r.jsx)(n.td,{children:"Angular coverage"}),(0,r.jsx)(n.td,{children:"-\u03c0 to +\u03c0 for 360\xb0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"min"}),"/",(0,r.jsx)(n.code,{children:"max"})," range"]}),(0,r.jsx)(n.td,{children:"Detection distance"}),(0,r.jsx)(n.td,{children:"0.1m - 30m"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"update_rate"})}),(0,r.jsx)(n.td,{children:"Scans per second"}),(0,r.jsx)(n.td,{children:"10-40 Hz"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"stddev"})," (noise)"]}),(0,r.jsx)(n.td,{children:"Range measurement error"}),(0,r.jsx)(n.td,{children:"0.01-0.05m"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"bridging-lidar-data-to-ros-2",children:"Bridging LiDAR Data to ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"Configure ros_gz_bridge to publish LiDAR data:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run ros_gz_bridge parameter_bridge \\\r\n    /scan@sensor_msgs/msg/LaserScan[gz.msgs.LaserScan\n"})}),"\n",(0,r.jsx)(n.p,{children:"Or in a launch file:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"Node(\r\n    package='ros_gz_bridge',\r\n    executable='parameter_bridge',\r\n    arguments=['/scan@sensor_msgs/msg/LaserScan[gz.msgs.LaserScan'],\r\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"visualizing-in-rviz",children:"Visualizing in RViz"}),"\n",(0,r.jsx)(n.p,{children:"View LiDAR data in RViz:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run rviz2 rviz2\n"})}),"\n",(0,r.jsx)(n.p,{children:"Then:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Set ",(0,r.jsx)(n.strong,{children:"Fixed Frame"})," to your LiDAR link (e.g., ",(0,r.jsx)(n.code,{children:"head_link"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:["Click ",(0,r.jsx)(n.strong,{children:"Add"})," \u2192 ",(0,r.jsx)(n.strong,{children:"By topic"})," \u2192 ",(0,r.jsx)(n.strong,{children:"/scan"})," \u2192 ",(0,r.jsx)(n.strong,{children:"LaserScan"})]}),"\n",(0,r.jsxs)(n.li,{children:["Adjust ",(0,r.jsx)(n.strong,{children:"Size"})," for point visibility"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You should see red dots showing where the laser beams hit surfaces."}),"\n",(0,r.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Depth cameras"})," (RGB-D cameras) provide both color images and per-pixel depth measurements. Common real-world examples include Intel RealSense and Microsoft Kinect."]}),"\n",(0,r.jsx)(n.h3,{id:"how-depth-cameras-work",children:"How Depth Cameras Work"}),"\n",(0,r.jsx)(n.p,{children:"Depth cameras typically use:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Structured light"}),": Project known patterns, analyze distortion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time-of-flight"}),": Measure light travel time per pixel"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo vision"}),": Triangulate depth from two cameras"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In simulation, Gazebo computes depth directly from the 3D scene\u2014no need to simulate the underlying technology."}),"\n",(0,r.jsx)(n.h3,{id:"adding-a-depth-camera",children:"Adding a Depth Camera"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="head_link">\r\n  <sensor name="rgbd_camera" type="rgbd_camera">\r\n    <pose>0.05 0 0 0 0 0</pose>  \x3c!-- 5cm forward on head --\x3e\r\n    <topic>camera</topic>\r\n    <update_rate>30</update_rate>\r\n\r\n    <camera>\r\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- ~60 degrees --\x3e\r\n      <image>\r\n        <width>640</width>\r\n        <height>480</height>\r\n        <format>R8G8B8</format>\r\n      </image>\r\n      <clip>\r\n        <near>0.1</near>\r\n        <far>10.0</far>\r\n      </clip>\r\n      <depth_camera>\r\n        <clip>\r\n          <near>0.1</near>\r\n          <far>10.0</far>\r\n        </clip>\r\n      </depth_camera>\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.007</stddev>\r\n      </noise>\r\n    </camera>\r\n\r\n    <always_on>true</always_on>\r\n    <visualize>true</visualize>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"depth-camera-topics",children:"Depth Camera Topics"}),"\n",(0,r.jsx)(n.p,{children:"A depth camera publishes multiple topics:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Topic"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Content"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/camera/image"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})}),(0,r.jsx)(n.td,{children:"Color image"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/camera/depth"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})}),(0,r.jsx)(n.td,{children:"Depth image (float32)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/camera/points"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/PointCloud2"})}),(0,r.jsx)(n.td,{children:"3D point cloud"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/camera/camera_info"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})}),(0,r.jsx)(n.td,{children:"Calibration data"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"bridge-configuration",children:"Bridge Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"Node(\r\n    package='ros_gz_bridge',\r\n    executable='parameter_bridge',\r\n    arguments=[\r\n        '/camera/image@sensor_msgs/msg/Image[gz.msgs.Image',\r\n        '/camera/depth@sensor_msgs/msg/Image[gz.msgs.Image',\r\n        '/camera/points@sensor_msgs/msg/PointCloud2[gz.msgs.PointCloudPacked',\r\n        '/camera/camera_info@sensor_msgs/msg/CameraInfo[gz.msgs.CameraInfo',\r\n    ],\r\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"viewing-point-clouds",children:"Viewing Point Clouds"}),"\n",(0,r.jsx)(n.p,{children:"In RViz:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Add ",(0,r.jsx)(n.strong,{children:"PointCloud2"})," display"]}),"\n",(0,r.jsxs)(n.li,{children:["Set topic to ",(0,r.jsx)(n.code,{children:"/camera/points"})]}),"\n",(0,r.jsxs)(n.li,{children:["Set ",(0,r.jsx)(n.strong,{children:"Fixed Frame"})," to camera frame"]}),"\n",(0,r.jsxs)(n.li,{children:["Adjust ",(0,r.jsx)(n.strong,{children:"Size"})," and ",(0,r.jsx)(n.strong,{children:"Color Transformer"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Point clouds show the 3D structure of the scene as your robot's camera perceives it\u2014essential for obstacle avoidance and manipulation."}),"\n",(0,r.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"IMU"})," (Inertial Measurement Unit) measures acceleration and angular velocity\u2014essential for balance control in humanoid robots."]}),"\n",(0,r.jsx)(n.h3,{id:"what-an-imu-measures",children:"What an IMU Measures"}),"\n",(0,r.jsx)(n.p,{children:"An IMU typically contains:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Measures"}),(0,r.jsx)(n.th,{children:"Units"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Accelerometer"}),(0,r.jsx)(n.td,{children:"Linear acceleration"}),(0,r.jsx)(n.td,{children:"m/s\xb2"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Gyroscope"}),(0,r.jsx)(n.td,{children:"Angular velocity"}),(0,r.jsx)(n.td,{children:"rad/s"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Magnetometer"}),(0,r.jsx)(n.td,{children:"Magnetic field"}),(0,r.jsx)(n.td,{children:"(optional, for heading)"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"For a humanoid robot, the IMU answers critical questions:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Am I falling? (sudden acceleration change)"}),"\n",(0,r.jsx)(n.li,{children:"How fast am I rotating? (gyroscope)"}),"\n",(0,r.jsx)(n.li,{children:"Which way is up? (gravity direction)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"adding-an-imu-sensor",children:"Adding an IMU Sensor"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="base_link">\r\n  <sensor name="imu_sensor" type="imu">\r\n    <pose>0 0 0 0 0 0</pose>\r\n    <topic>imu</topic>\r\n    <update_rate>200</update_rate>  \x3c!-- High rate for balance control --\x3e\r\n\r\n    <imu>\r\n      <angular_velocity>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0002</stddev>\r\n          </noise>\r\n        </x>\r\n        <y>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0002</stddev>\r\n          </noise>\r\n        </y>\r\n        <z>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0002</stddev>\r\n          </noise>\r\n        </z>\r\n      </angular_velocity>\r\n      <linear_acceleration>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.017</stddev>\r\n          </noise>\r\n        </x>\r\n        <y>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.017</stddev>\r\n          </noise>\r\n        </y>\r\n        <z>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.017</stddev>\r\n          </noise>\r\n        </z>\r\n      </linear_acceleration>\r\n    </imu>\r\n\r\n    <always_on>true</always_on>\r\n    <visualize>false</visualize>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"imu-data-in-ros-2",children:"IMU Data in ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"Bridge the IMU data:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run ros_gz_bridge parameter_bridge \\\r\n    /imu@sensor_msgs/msg/Imu[gz.msgs.IMU\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"sensor_msgs/msg/Imu"})," message contains:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"orientation: quaternion (x, y, z, w)\r\nangular_velocity: vector (x, y, z) in rad/s\r\nlinear_acceleration: vector (x, y, z) in m/s\xb2\r\ncovariance matrices for each\n"})}),"\n",(0,r.jsx)(n.h3,{id:"using-imu-for-balance",children:"Using IMU for Balance"}),"\n",(0,r.jsx)(n.p,{children:"In a humanoid controller, IMU data feeds into the balance system:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def balance_callback(self, imu_msg):\r\n    # Extract orientation (quaternion -> euler)\r\n    orientation = imu_msg.orientation\r\n    roll, pitch, yaw = quaternion_to_euler(orientation)\r\n\r\n    # Check if robot is tilting too much\r\n    if abs(pitch) > 0.1:  # ~6 degrees\r\n        self.get_logger().warn(f'Robot tilting! Pitch: {pitch:.2f} rad')\r\n        self.apply_correction(pitch)\r\n\r\n    # Use angular velocity for rate feedback\r\n    pitch_rate = imu_msg.angular_velocity.y\r\n    # ... feed into PD controller for balance\n"})}),"\n",(0,r.jsx)(n.h2,{id:"sensor-noise-and-realism",children:"Sensor Noise and Realism"}),"\n",(0,r.jsx)(n.p,{children:"Perfect sensors don't exist. Real sensors have noise, drift, and systematic errors. To prepare your algorithms for reality, you must add realistic noise to simulated sensors."}),"\n",(0,r.jsx)(n.h3,{id:"types-of-sensor-noise",children:"Types of Sensor Noise"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Noise Type"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Effect"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Gaussian"}),(0,r.jsx)(n.td,{children:"Random variations around true value"}),(0,r.jsx)(n.td,{children:"Most common, easy to model"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Bias"}),(0,r.jsx)(n.td,{children:"Constant offset from true value"}),(0,r.jsx)(n.td,{children:"Causes drift over time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Scale factor"}),(0,r.jsx)(n.td,{children:"Multiplicative error"}),(0,r.jsx)(n.td,{children:"Distances consistently over/under-estimated"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Quantization"}),(0,r.jsx)(n.td,{children:"Discrete resolution steps"}),(0,r.jsx)(n.td,{children:"Visible in low-resolution sensors"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Temporal"}),(0,r.jsx)(n.td,{children:"Changes over time (drift)"}),(0,r.jsx)(n.td,{children:"IMU orientation error accumulates"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"configuring-noise-in-gazebo",children:"Configuring Noise in Gazebo"}),"\n",(0,r.jsx)(n.p,{children:"Gazebo supports Gaussian noise on most sensors:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<noise>\r\n  <type>gaussian</type>\r\n  <mean>0.0</mean>      \x3c!-- Bias: usually 0 for unbiased sensors --\x3e\r\n  <stddev>0.01</stddev> \x3c!-- Standard deviation of noise --\x3e\r\n</noise>\n"})}),"\n",(0,r.jsx)(n.p,{children:"Typical noise values:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sensor"}),(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Typical Noise"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LiDAR"}),(0,r.jsx)(n.td,{children:"Range"}),(0,r.jsx)(n.td,{children:"0.01 - 0.03 m"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Camera"}),(0,r.jsx)(n.td,{children:"Pixel intensity"}),(0,r.jsx)(n.td,{children:"0.005 - 0.02"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"IMU (gyro)"}),(0,r.jsx)(n.td,{children:"Angular velocity"}),(0,r.jsx)(n.td,{children:"0.0001 - 0.001 rad/s"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"IMU (accel)"}),(0,r.jsx)(n.td,{children:"Linear acceleration"}),(0,r.jsx)(n.td,{children:"0.01 - 0.05 m/s\xb2"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"domain-randomization-for-sensors",children:"Domain Randomization for Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Beyond fixed noise, domain randomization varies sensor parameters during training:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Pseudocode for sensor domain randomization\r\nfor each training episode:\r\n    lidar_noise_stddev = random.uniform(0.005, 0.05)\r\n    camera_brightness = random.uniform(0.8, 1.2)\r\n    imu_bias = random.uniform(-0.01, 0.01)\r\n\r\n    # Configure simulation with randomized sensor parameters\r\n    update_sensor_config(lidar_noise=lidar_noise_stddev, ...)\r\n\r\n    # Train with this configuration\r\n    run_episode()\n"})}),"\n",(0,r.jsx)(n.p,{children:"This teaches your perception algorithms to handle sensor variations they'll encounter in the real world."}),"\n",(0,r.jsx)(n.h3,{id:"the-sim-to-real-sensor-gap",children:"The Sim-to-Real Sensor Gap"}),"\n",(0,r.jsx)(n.p,{children:"Simulated sensors differ from real ones in several ways:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Simulation"}),(0,r.jsx)(n.th,{children:"Reality"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Noise distribution"}),(0,r.jsx)(n.td,{children:"Perfect Gaussian"}),(0,r.jsx)(n.td,{children:"Complex, correlated"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Environmental effects"}),(0,r.jsx)(n.td,{children:"Often ignored"}),(0,r.jsx)(n.td,{children:"Reflections, glare, fog"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Sensor artifacts"}),(0,r.jsx)(n.td,{children:"Clean data"}),(0,r.jsx)(n.td,{children:"Motion blur, rolling shutter"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Calibration"}),(0,r.jsx)(n.td,{children:"Perfect"}),(0,r.jsx)(n.td,{children:"Requires careful procedure"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Strategies to bridge this gap:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Add realistic noise models"})," based on sensor datasheets"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use domain randomization"})," to build robustness"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine-tune on real data"})," after simulation training"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Test with sensor-realistic synthetic data"})," (photorealistic renderers)"]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"Key Insight",type:"tip",children:(0,r.jsxs)(n.p,{children:["The goal isn't to perfectly match reality in simulation\u2014that's impossible. The goal is to build algorithms robust enough to handle the ",(0,r.jsx)(n.em,{children:"range"})," of conditions they might encounter, including real-world sensor imperfections."]})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you learned how to add simulated sensors to your digital twin:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Concepts:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sensor simulation completes the perception-action loop"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Without sensors, you can only test actuation"}),"\n",(0,r.jsx)(n.li,{children:"With sensors, you can develop full perception pipelines"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LiDAR sensors"})," provide distance measurements"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Configure with samples, angle range, noise"}),"\n",(0,r.jsx)(n.li,{children:"Visualize as LaserScan in RViz"}),"\n",(0,r.jsx)(n.li,{children:"Essential for navigation and obstacle avoidance"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Depth cameras"})," provide RGB + depth data"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Output: images, depth maps, point clouds"}),"\n",(0,r.jsx)(n.li,{children:"Configure resolution, field of view, noise"}),"\n",(0,r.jsx)(n.li,{children:"Essential for manipulation and scene understanding"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"IMU sensors"})," measure acceleration and rotation"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Critical for humanoid balance control"}),"\n",(0,r.jsx)(n.li,{children:"High update rate (100-400 Hz typical)"}),"\n",(0,r.jsx)(n.li,{children:"Prone to drift\u2014often fused with other sensors"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensor noise"})," bridges simulation to reality"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Add Gaussian noise to all sensors"}),"\n",(0,r.jsx)(n.li,{children:"Use domain randomization for robustness"}),"\n",(0,r.jsx)(n.li,{children:"Real sensors have complex noise patterns"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Virtual Rehearsal Extended:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Human Sense"}),(0,r.jsx)(n.th,{children:"Robot Sensor"}),(0,r.jsx)(n.th,{children:"Simulation"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Vision"}),(0,r.jsx)(n.td,{children:"Camera/LiDAR"}),(0,r.jsx)(n.td,{children:"Image/point cloud plugins"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Balance"}),(0,r.jsx)(n.td,{children:"IMU"}),(0,r.jsx)(n.td,{children:"Acceleration/gyro data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Proprioception"}),(0,r.jsx)(n.td,{children:"Joint encoders"}),(0,r.jsx)(n.td,{children:"Joint state publisher"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Touch"}),(0,r.jsx)(n.td,{children:"Force sensors"}),(0,r.jsx)(n.td,{children:"Contact plugins"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What's Next:"})}),"\n",(0,r.jsxs)(n.p,{children:["Your digital twin can now move ",(0,r.jsx)(n.em,{children:"and"})," perceive. But Gazebo prioritizes physics accuracy over visual realism. In Chapter 4, we'll explore Unity\u2014a game engine that provides photorealistic rendering for training vision systems and creating human-robot interaction scenarios."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.admonition,{title:"Next Chapter",type:"tip",children:(0,r.jsxs)(n.p,{children:["Continue to ",(0,r.jsx)(n.a,{href:"./unity-visualization-hri",children:"Chapter 4: High-Fidelity Visualization and Interaction with Unity"})," to learn about advanced visualization and human-robot interaction."]})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var s=i(6540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);